{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWYgqaRZN6X4HY8GSJVdsV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satyamverma95/DL_NLP/blob/main/Code_Explanation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIIWIw8fbEG_"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=384,\n",
        "        truncation=\"only_second\",\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    answers = examples[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        answer = answers[i]\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "        # Find the start and end of the context\n",
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        while sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "        # If the answer is not fully inside the context, label it (0, 0)\n",
        "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            # Otherwise it's the start and end token positions\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples[\"context\"],\n",
        "        max_length=384,\n",
        "        truncation=\"only_second\",\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n"
      ],
      "metadata": {
        "id": "0SxY8kFgbNC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The function preprocess_function takes a dictionary examples as input.</br>\n",
        "- It extracts the \"question\" field from examples and stores each question in a list called questions. The strip() method is used to remove any leading or trailing whitespace from each question.</br>\n",
        "- The tokenizer is then called with the following parameters:</br>\n",
        "- The questions list as the first argument.</br>\n",
        "- The \"context\" field from examples as the second argument.</br>\n",
        "- max_length=384 sets the maximum length of the tokenized inputs to 384 tokens.\n",
        "truncation=\"only_second\" truncates the context if it exceeds the maximum length, keeping the question intact.</br>\n",
        "- return_offsets_mapping=True instructs the tokenizer to return the mapping between the tokens and the original character offsets.</br>\n",
        "- padding=\"max_length\" pads the inputs to the maximum length."
      ],
      "metadata": {
        "id": "XmjpnxuVbgn3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) max_length=384 sets the maximum length of the tokenized inputs to 384 tokens.\n",
        "Explain this with example\n",
        "\n",
        "Ans) Certainly! The max_length=384 parameter in the code sets the maximum length of the tokenized inputs to 384 tokens. This means that after tokenization, the resulting input sequence will be truncated or padded to a maximum length of 384 tokens.\n",
        "\n",
        "Let's consider an example to understand this better:\n",
        "\n",
        "Context: \"The quick brown fox jumps over the lazy dog.\"\n",
        "Question: \"What color is the fox?\"\n",
        "\n",
        "To tokenize this example, we'll use a hypothetical tokenizer that splits the input into individual words:\n",
        "\n",
        "Tokenized context: [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\", \".\"]\n",
        "Tokenized question: [\"What\", \"color\", \"is\", \"the\", \"fox\", \"?\"]\n",
        "\n",
        "Now, let's see how the max_length=384 parameter affects the tokenized inputs.\n",
        "\n",
        "In this example, both the context and the question have very few tokens, so the resulting input sequence length will be well below the maximum length of 384 tokens. Therefore, no truncation or padding is required.\n",
        "\n",
        "However, if we consider a longer context and question, such as:\n",
        "\n",
        "Context: \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\"\n",
        "Question: \"What is the meaning of Lorem ipsum?\"\n",
        "\n",
        "After tokenization, the input sequences would look like this:\n",
        "\n",
        "Tokenized context: [\"Lorem\", \"ipsum\", \"dolor\", \"sit\", \"amet\", \",\", \"consectetur\", \"adipiscing\", \"elit\", \",\", \"sed\", \"do\", \"eiusmod\", \"tempor\", \"incididunt\", \"ut\", \"labore\", \"et\", \"dolore\", \"magna\", \"aliqua\", \".\"]\n",
        "Tokenized question: [\"What\", \"is\", \"the\", \"meaning\", \"of\", \"Lorem\", \"ipsum\", \"?\"]\n",
        "\n",
        "In this case, the total number of tokens in the context and question combined exceeds 384 tokens. Therefore, truncation or padding will be applied to ensure that the total length does not exceed the maximum length.\n",
        "\n",
        "Truncation: If the total length exceeds 384 tokens, the tokenizer will truncate the input sequence by removing tokens from the end of the sequence until it reaches the maximum length.\n",
        "\n",
        "Padding: If the total length is less than 384 tokens, the tokenizer will add special padding tokens (e.g., <pad>) to the input sequence until it reaches the maximum length.\n",
        "\n",
        "By setting max_length=384, the code ensures that the tokenized inputs are limited to a maximum length of 384 tokens, allowing for consistent input size during model training or evaluation. "
      ],
      "metadata": {
        "id": "L1Oa3SQ0c5er"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) return_offsets_mapping=True, Explain this with example?\n",
        "Ans) Certainly! The return_offsets_mapping=True parameter in the code instructs the tokenizer to return the mapping between the tokens and the original character offsets in the input text. This mapping provides the information about which characters in the original text correspond to each token in the tokenized sequence.\n",
        "\n",
        "Let's consider an example to understand this better:\n",
        "\n",
        "Context: \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "Tokenized context: [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\", \".\"]\n",
        "\n",
        "When return_offsets_mapping=True, the tokenizer will also return the offsets mapping, which is a list of tuples representing the start and end character offsets for each token.\n",
        "\n",
        "Offsets mapping: [(0, 3), (4, 9), (10, 15), (16, 19), (20, 25), (26, 30), (31, 34), (35, 39), (40, 43), (43, 44)]\n",
        "\n",
        "Each tuple in the offsets mapping represents the start and end character offsets for a token. For example, the first tuple (0, 3) corresponds to the token \"The\", which starts at character index 0 and ends at character index 3 in the original text.\n",
        "\n",
        "This offsets mapping provides a way to align the tokens back to their original positions in the input text. It can be useful for tasks like extracting answers from a given context or highlighting specific tokens in the original text.\n",
        "\n",
        "In the context of the provided code, the offsets mapping is used to determine the character offsets of the answer span in the original text and label the corresponding start and end positions of the answer span in the tokenized sequence.\n",
        "\n",
        "Overall, by setting return_offsets_mapping=True, the code enables the retrieval of the offsets mapping, allowing for better alignment between tokens and the original text, which can be helpful in various downstream tasks."
      ],
      "metadata": {
        "id": "rSBDVwj7dq3-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) truncation=\"only_second\", Explain with example?\n",
        "Ans) The `truncation=\"only_second\"` parameter in the code specifies the truncation strategy to be applied when tokenizing the inputs. In this case, it means that if the combined length of the question and the context exceeds the maximum length allowed (`max_length=384`), only the context will be truncated, and the question will be kept intact.\n",
        "\n",
        "Let's consider an example to understand this better:\n",
        "\n",
        "Context: \"The quick brown fox jumps over the lazy dog.\"\n",
        "Question: \"What color is the fox?\"\n",
        "\n",
        "Tokenized context: [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\", \".\"]\n",
        "Tokenized question: [\"What\", \"color\", \"is\", \"the\", \"fox\", \"?\"]\n",
        "\n",
        "In this example, the combined length of the tokenized question and the tokenized context is 11. If the maximum length is set to 10, truncation will be necessary to fit the inputs within the specified maximum length.\n",
        "\n",
        "With `truncation=\"only_second\"`, only the context will be truncated, and the question will be kept intact. The resulting tokenized inputs will look like this:\n",
        "\n",
        "Truncated tokenized context: [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
        "Tokenized question: [\"What\", \"color\", \"is\", \"the\", \"fox\", \"?\"]\n",
        "\n",
        "As you can see, the context has been truncated by removing the last token \".\", while the question remains unchanged.\n",
        "\n",
        "By using `truncation=\"only_second\"`, the code ensures that the question is preserved as much as possible, even if it means truncating the context. This can be useful in question answering tasks, where the question plays a crucial role in understanding the context and generating accurate answers."
      ],
      "metadata": {
        "id": "SEl1MaCCej9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "    answers = examples[\"answers\"]\n",
        "    start_positions = []\n",
        "    end_positions = []\n"
      ],
      "metadata": {
        "id": "jdiPt-Fye7Dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"offset_mapping\" is extracted from the inputs dictionary using the pop() method and assigned to the offset_mapping variable. This mapping represents the correspondence between the tokens and the original character offsets in the context.\n",
        "The \"answers\" field from examples is assigned to the answers variable.\n",
        "Empty lists, start_positions and end_positions, are created to store the start and end positions of the answer."
      ],
      "metadata": {
        "id": "yuvPMOEDfHNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    for i, offset in enumerate(offset_mapping):\n",
        "        answer = answers[i]\n",
        "        start_char = answer[\"answer_start\"][0]\n",
        "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "        sequence_ids = inputs.sequence_ids(i)\n"
      ],
      "metadata": {
        "id": "C5FSRyj3f8lT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A loop is started to iterate over each element in offset_mapping. i represents the index and offset represents the corresponding offset mapping for that index.</br>\n",
        "- The answer variable is assigned the i-th element from the answers list.</br>\n",
        "- The start and end character positions of the answer are calculated using the \"answer_start\" field of answer. In this example, start_char is 515 and end_char is 515 + length of the answer text.</br>\n",
        "- The sequence_ids are obtained from the inputs using the sequence_ids() method, passing the index i.</br>"
      ],
      "metadata": {
        "id": "6cAYX3XBgBRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Example of OFFSET Mapping?\n",
        "ANS) Sure! Here's an example of offset mapping for the given context and question:\n",
        "\n",
        "Context: \"Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary.\"\n",
        "\n",
        "Question: \"To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\"\n",
        "\n",
        "Tokenized context: [\"Architecturally\", \",\", \"the\", \"school\", \"has\", \"a\", \"Catholic\", \"character\", \".\", \"Atop\", \"the\", \"Main\", \"Building\", \"'\", \"s\", \"gold\", \"dome\", \"is\", \"a\", \"golden\", \"statue\", \"of\", \"the\", \"Virgin\", \"Mary\", \".\"]\n",
        "\n",
        "Tokenized question: [\"To\", \"whom\", \"did\", \"the\", \"Virgin\", \"Mary\", \"allegedly\", \"appear\", \"in\", \"1858\", \"in\", \"Lourdes\", \"France\", \"?\"]\n",
        "\n",
        "Offset mapping: [(0, 15), (16, 16), (17, 19), (20, 25), (26, 29), (30, 31), (32, 39), (40, 48), (48, 49), (50, 54), (55, 57), (58, 64), (65, 72), (73, 74), (75, 76), (77, 81), (82, 86), (87, 88), (89, 90), (91, 97), (98, 103), (104, 106), (107, 110), (111, 116), (117, 121), (121, 122)]\n",
        "\n",
        "The offset mapping represents the character-level mapping between the original text and the tokenized inputs. Each tuple in the offset mapping represents the start and end character positions of a token in the original text. For example, the first token \"Architecturally\" corresponds to characters 0 to 15 in the original context. The token \"Mary\" corresponds to characters 117 to 121.\n",
        "\n",
        "The offset mapping is useful for aligning the tokenized outputs with the original text. It allows you to retrieve the character-level positions of specific tokens or spans in the text, which can be helpful when generating answers or evaluating model performance."
      ],
      "metadata": {
        "id": "ocbpp-Y-gc7W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) sequence_ids = inputs.sequence_ids(i), Explain with exmaple?\n",
        "Ans) The line `sequence_ids = inputs.sequence_ids(i)` retrieves the sequence IDs for a particular example in the input batch. In this case, the `inputs` object represents the tokenized inputs.\n",
        "\n",
        "Let's consider an example to understand this line:\n",
        "\n",
        "Context: \"Architecturally, the school has a Catholic character.\"\n",
        "Question: \"What is the character of the school?\"\n",
        "\n",
        "Tokenized context: [\"Architecturally\", \",\", \"the\", \"school\", \"has\", \"a\", \"Catholic\", \"character\", \".\"]\n",
        "Tokenized question: [\"What\", \"is\", \"the\", \"character\", \"of\", \"the\", \"school\", \"?\"]\n",
        "\n",
        "In this example, suppose we are processing the second example in the batch (i.e., `i = 1`). The tokenized question is `[\"What\", \"is\", \"the\", \"character\", \"of\", \"the\", \"school\", \"?\"]`.\n",
        "\n",
        "The `sequence_ids` will be a list representing the sequence of IDs for the tokens in the question. Each ID corresponds to a specific segment of the input. In this case, the IDs can be represented as follows:\n",
        "\n",
        "sequence_ids: [0, 0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "Here, all the tokens in the question belong to segment 0 since the question is part of the first sequence in the input.\n",
        "\n",
        "The `sequence_ids` can be useful for identifying the segment to which each token belongs, especially in cases where the input has multiple segments or different types of inputs (e.g., question and context).\n",
        "\n",
        "In the provided code, the `sequence_ids` are used to determine the start and end positions of the context in the input, which are important for identifying the answer spans within the context."
      ],
      "metadata": {
        "id": "Yb2fzld-jBC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        while sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n"
      ],
      "metadata": {
        "id": "YUlsMJSDjeZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Let's go through the code line by line with an example:\n",
        "\n",
        "Consider the following tokenized context and sequence_ids:\n",
        "\n",
        "Tokenized context: [\"I\", \"love\", \"cats\", \".\"]\n",
        "sequence_ids: [0, 0, 0, 0]\n",
        "\n",
        "1. `idx = 0`: Initialize the variable idx to 0, which will be used as an index to traverse the sequence_ids list.\n",
        "\n",
        "2. `while sequence_ids[idx] != 1:`: Enter a while loop that continues until the value at sequence_ids[idx] is not equal to 1. This loop is used to find the start position of the context within the sequence_ids.\n",
        "\n",
        "   In our example, since all the tokens in the context have a sequence ID of 0, the loop condition is True for all iterations. The loop will iterate until it encounters a token with a sequence ID of 1.\n",
        "\n",
        "3. `idx += 1`: Increment the idx variable to move to the next index in the sequence_ids list.\n",
        "\n",
        "4. `context_start = idx`: Assign the value of idx to the context_start variable. This represents the index of the first token that belongs to the context.\n",
        "\n",
        "   In our example, the loop will iterate 4 times (equal to the length of the sequence_ids list) before the condition becomes False. Therefore, context_start will be assigned the value of 4, indicating that the context starts at index 4 in the tokenized sequence.\n",
        "\n",
        "5. `while sequence_ids[idx] == 1:`: Enter another while loop that continues until the value at sequence_ids[idx] is equal to 1. This loop is used to find the end position of the context within the sequence_ids.\n",
        "\n",
        "   In our example, since all the tokens in the context have a sequence ID of 0, the loop condition is False for all iterations. The loop will not be entered, and the value of idx remains the same as in the previous step.\n",
        "\n",
        "6. `context_end = idx - 1`: Subtract 1 from idx and assign the result to the context_end variable. This represents the index of the last token that belongs to the context.\n",
        "\n",
        "   In our example, context_end will be assigned the value of 3 since idx remains unchanged. This indicates that the context ends at index 3 in the tokenized sequence.\n",
        "\n",
        "The code snippet is used to find the start and end positions of the context within the sequence_ids list. These positions are then used to determine the boundaries of the context and extract relevant information, such as answer spans, from the tokenized inputs."
      ],
      "metadata": {
        "id": "G7GFGrgejgd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n"
      ],
      "metadata": {
        "id": "lt-HuQXrk3ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- An if condition checks if the start character position of the answer is outside the context or if the end character position is before the context. If either condition is true, the answer is labeled as (0, 0) and appended to start_positions and end_positions as 0.</br>\n",
        "- Otherwise, the start position index is calculated by incrementing idx until it reaches the end of the context or the token offset exceeds the start character position.</br>\n",
        "- The start_positions list is appended with idx - 1, representing the token index of the start position.</br>\n",
        "- Similarly, the end position index is calculated by decrementing idx until it reaches the start of the context or the token offset is before the end character position.</br>\n",
        "- The end_positions list is appended with idx + 1, representing the token index of the end position.</br>"
      ],
      "metadata": {
        "id": "SLqFVXJsk6ww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2nd Explanation:\n",
        "\n",
        "Let's walk through the code line by line with an example:\n",
        "\n",
        "Consider the following offset mapping:\n",
        "\n",
        "offset_mapping: [(0, 0), (2, 5), (6, 10), (11, 12)]\n",
        "\n",
        "1. `if offset[context_start][0] > end_char or offset[context_end][1] < start_char:`: Check if the start position of the context is greater than the end character position or if the end position of the context is less than the start character position. This condition is used to handle cases where the answer is not fully inside the context.\n",
        "\n",
        "   In our example, offset[context_start][0] is 0 and offset[context_end][1] is 12. The start_char is 515 and end_char is 536. Since both conditions are true (0 > 536 and 12 < 515), the code enters the if block.\n",
        "\n",
        "2. `start_positions.append(0)` and `end_positions.append(0)`: If the answer is not fully inside the context, append 0 to both the start_positions and end_positions lists. This indicates that the answer is not present in the context.\n",
        "\n",
        "   In our example, both start_positions and end_positions will have a new entry of 0 appended to them.\n",
        "\n",
        "3. `else:`: If the answer is fully inside the context, execute the code block inside the else statement.\n",
        "\n",
        "4. `idx = context_start`: Initialize the idx variable with the value of context_start.\n",
        "\n",
        "   In our example, idx is assigned the value of 4.\n",
        "\n",
        "5. `while idx <= context_end and offset[idx][0] <= start_char:`: Enter a while loop that continues until idx is less than or equal to the context_end and the start position of the current token is less than or equal to the start character position of the answer.\n",
        "\n",
        "   In our example, the loop condition is False since the start position of the first token (0) is greater than the start_char (515). The loop is not executed.\n",
        "\n",
        "6. `start_positions.append(idx - 1)`: Append the value of idx - 1 to the start_positions list. This represents the token index where the answer span starts within the context.\n",
        "\n",
        "   In our example, since the loop is not executed, idx remains unchanged. Therefore, start_positions will have a new entry of 3 appended to it.\n",
        "\n",
        "7. `idx = context_end`: Set idx to the value of context_end.\n",
        "\n",
        "   In our example, idx is assigned the value of 3.\n",
        "\n",
        "8. `while idx >= context_start and offset[idx][1] >= end_char:`: Enter a while loop that continues until idx is greater than or equal to the context_start and the end position of the current token is greater than or equal to the end character position of the answer.\n",
        "\n",
        "   In our example, the loop condition is False since the end position of the last token (12) is less than the end_char (536). The loop is not executed.\n",
        "\n",
        "9. `end_positions.append(idx + 1)`: Append the value of idx + 1 to the end_positions list. This represents the token index where the answer span ends within the context.\n",
        "\n",
        "   In our example, since the loop is not executed, idx remains unchanged. Therefore, end_positions will have a new entry of 4 appended to it.\n",
        "\n",
        "The code snippet is used to determine the start and end positions of the answer span within the context based on the offset mapping. If the answer is fully inside the context, the corresponding token indices are added to the start_positions and end_positions lists. If the answer is not fully inside the context, both start_positions and end_positions are assigned a value of 0 to indicate that the answer is not present in the context."
      ],
      "metadata": {
        "id": "k0uaLbi9mx98"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ctNzLRGMjgHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    inputs[\"start_positions\"] = start_positions\n",
        "    inputs[\"end_positions\"] = end_positions\n",
        "    return inputs\n"
      ],
      "metadata": {
        "id": "K5Aa4FibbfSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The start_positions and end_positions lists are assigned as values to the respective keys in the inputs dictionary.</br>\n",
        "- Finally, the inputs dictionary, containing the tokenized inputs along with the start and end positions, is returned from the preprocess_function.</br>"
      ],
      "metadata": {
        "id": "IS7Q0aaNmcx5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6) Explain Input ID logic?\n",
        "\n",
        "Ans) The `input_ids` represent the tokenized and encoded input sequences, which are an essential part of natural language processing (NLP) models. Let's dive into the logic behind `input_ids`.\n",
        "\n",
        "In NLP, text data needs to be transformed into numerical representations that can be processed by machine learning models. Tokenization is the process of breaking down the input text into individual tokens or subwords. Each token typically corresponds to a word, subword, or character.\n",
        "\n",
        "The `input_ids` list contains the token IDs for each token in the input sequence. These token IDs are specific to the tokenizer used and are usually mapped from a vocabulary. Each token ID represents a unique identifier for a particular token.\n",
        "\n",
        "For example, consider the following input sentence: \"I love natural language processing.\" After tokenization, it may be represented as the following list of tokens: [\"I\", \"love\", \"natural\", \"language\", \"processing\"]. The tokenizer will assign a numerical ID to each token based on its position in the vocabulary. Let's assume the vocabulary assigns the following IDs: \"I\" (10), \"love\" (24), \"natural\" (37), \"language\" (42), and \"processing\" (52).\n",
        "\n",
        "The corresponding `input_ids` for this example would be: [10, 24, 37, 42, 52]. Each ID in the `input_ids` list represents the token ID for the respective token in the input sequence.\n",
        "\n",
        "The `input_ids` are crucial for feeding the tokenized input sequence into a machine learning model. The model processes these numerical representations as inputs and performs computations such as embedding lookups, attention mechanisms, and neural network operations on these token IDs to derive meaningful representations for the given text.\n",
        "\n",
        "It's important to note that the specific tokenization and vocabulary used by the tokenizer can vary depending on the model and tokenizer implementation. Therefore, the token IDs in `input_ids` may differ across different models or tokenizers."
      ],
      "metadata": {
        "id": "Vnrrninw-loo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7) What is input.sequence_ids()?\n",
        "Ans) The `inputs.sequence_ids` is a method provided by the Hugging Face tokenizer that returns the sequence IDs for a specific example in the input batch. Sequence IDs are used in models that support sequence classification tasks, where different segments of the input sequence may need to be identified or differentiated.\n",
        "\n",
        "Let's explain with an example to understand the `inputs.sequence_ids` method:\n",
        "\n",
        "Assume you have the following batch of input sequences:\n",
        "\n",
        "```python\n",
        "input_batch = [\n",
        "    (\"What is the capital of France?\", \"Paris is the capital of France.\"),\n",
        "    (\"Who wrote Harry Potter?\", \"J.K. Rowling wrote the Harry Potter series.\")\n",
        "]\n",
        "```\n",
        "\n",
        "You can tokenize and encode this batch using the tokenizer:\n",
        "\n",
        "```python\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "inputs = tokenizer(\n",
        "    input_batch,\n",
        "    padding=\"longest\",\n",
        "    truncation=True,\n",
        "    max_length=128,\n",
        "    return_offsets_mapping=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "```\n",
        "\n",
        "The `inputs.sequence_ids(i)` method can be used to retrieve the sequence IDs for the `i`-th example in the batch. It helps identify different segments or parts of the input sequence. Here's how it works:\n",
        "\n",
        "```python\n",
        "for i, offset in enumerate(inputs[\"offset_mapping\"]):\n",
        "    answer = answers[i]\n",
        "    start_char = answer[\"answer_start\"][0]\n",
        "    end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
        "    sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "    print(\"Iterating for each Offset Mapping : i-{}, offset-{}, sequence_ids-{}\".format(i, offset, sequence_ids))\n",
        "```\n",
        "\n",
        "The output would look like:\n",
        "\n",
        "```\n",
        "Iterating for each Offset Mapping : i-0, offset-[(0, 0), (0, 4), (5, 7), (8, 9), (10, 16), (17, 19), (20, 23), (23, 24), (25, 28), (29, 33), (0, 0), (0, 0), (0, 0)], sequence_ids-tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 2])\n",
        "Iterating for each Offset Mapping : i-1, offset-[(0, 0), (0, 3), (4, 9), (10, 17), (18, 23), (24, 27), (28, 31), (32, 35), (36, 42), (43, 50), (0, 0), (0, 0), (0, 0)], sequence_ids-tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 2])\n",
        "```\n",
        "\n",
        "In this example, the `sequence_ids` indicate the segment or part of the input sequence to which each token belongs. The numbers in the `sequence_ids` tensor represent different segments, with each segment having a unique identifier. In this case, segment 0 corresponds to the question, segment 1 corresponds to the context, and segment 2 corresponds to the padding tokens.\n",
        "\n",
        "Note that the specific values assigned to the sequence IDs may vary depending on the tokenizer and the specific model used. It is important to consult the documentation or the specific model's implementation to understand the semantics of the sequence IDs in more detail."
      ],
      "metadata": {
        "id": "XG_wZ-AhCBHd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8) Explain the below code :\n",
        "\n",
        "```python\n",
        "if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
        "  start_positions.append(0)\n",
        "  end_positions.append(0)\n",
        "```\n",
        "\n",
        "Ans) The code you provided checks if the character offsets of the context in the `offset_mapping` are within the range of the answer span defined by the `start_char` and `end_char`. Let's break down the logic with an example:\n",
        "\n",
        "Assume you have the following example:\n",
        "\n",
        "```python\n",
        "context = \"Paris is the capital of France.\"\n",
        "answer = {\n",
        "    \"answer_start\": [6],\n",
        "    \"text\": [\"capital\"]\n",
        "}\n",
        "```\n",
        "\n",
        "The `offset_mapping` represents the character offsets of each token in the context. For this example, let's assume the `offset_mapping` is as follows:\n",
        "\n",
        "```python\n",
        "offset_mapping = [\n",
        "    (0, 0),     # [CLS]\n",
        "    (0, 5),     # Paris\n",
        "    (6, 8),     # is\n",
        "    (9, 12),    # the\n",
        "    (13, 20),   # capital\n",
        "    (21, 23),   # of\n",
        "    (24, 30),   # France\n",
        "    (30, 31),   # .\n",
        "    (0, 0),     # [PAD]\n",
        "    (0, 0)      # [PAD]\n",
        "]\n",
        "```\n",
        "\n",
        "In the code snippet you provided, it checks if the character offsets of the context span (denoted by `offset[context_start][0]` and `offset[context_end][1]`) are within the range of the answer span defined by `start_char` and `end_char`.\n",
        "\n",
        "In this example, `start_char` is 6 and `end_char` is 13, which correspond to the start and end positions of the word \"capital\" in the context. The `offset[context_start][0]` represents the start character offset of the context span, and `offset[context_end][1]` represents the end character offset of the context span.\n",
        "\n",
        "The conditions being checked are:\n",
        "\n",
        "1. `offset[context_start][0] > end_char`: This condition checks if the start character offset of the context span is greater than the `end_char` (end character offset of the answer span). If this condition is true, it means that the context span starts after the answer span, indicating that the answer is not present in the given context. In such cases, the `start_positions` and `end_positions` are appended with 0, indicating there is no answer.\n",
        "\n",
        "2. `offset[context_end][1] < start_char`: This condition checks if the end character offset of the context span is less than the `start_char` (start character offset of the answer span). If this condition is true, it means that the context span ends before the answer span, indicating that the answer is not present in the given context. Again, 0 is appended to `start_positions` and `end_positions` to indicate no answer.\n",
        "\n",
        "In our example, the condition `offset[context_start][0] > end_char` is `False` because `offset[6][0]` (which corresponds to the start character offset of the word \"capital\") is not greater than `end_char` (13). Similarly, the condition `offset[context_end][1] < start_char` is also `False` because `offset[20][1]` (which corresponds to the end character offset of the word \"capital\") is not less than `start_char` (6).\n",
        "\n",
        "Therefore, in this case, since both conditions are `False`, the code does not append 0 to `start_positions` and `end_positions`, indicating that there is an answer present in the given context span."
      ],
      "metadata": {
        "id": "MYzpQ_D_P0cm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9) What is the purpose of attention mask?</br>\n",
        "Ans)The purpose of an attention mask in the context of models like BERT (Bidirectional Encoder Representations from Transformers) is to indicate which tokens in the input should be attended to and which should be ignored during the self-attention mechanism.\n",
        "\n",
        "The attention mask is a binary mask of shape `[batch_size, sequence_length]`, where `batch_size` is the number of input examples and `sequence_length` is the length of the input sequence. It has the following components:\n",
        "\n",
        "- Value 1: Tokens that should be attended to, i.e., not ignored.\n",
        "- Value 0: Tokens that should be ignored and not attended to.\n",
        "\n",
        "The attention mask serves two main purposes:\n",
        "\n",
        "1. Masking Padded Tokens: When the input sequences have different lengths within a batch, the shorter sequences are padded with special tokens (such as `[PAD]`) to match the length of the longest sequence. The attention mask helps the model distinguish between the actual tokens and the padding tokens, preventing the model from attending to the padding tokens and avoiding any unnecessary computation or influence on the model's predictions.\n",
        "\n",
        "2. Masking Future Tokens: In a language model, during the training or inference phase, the model should not have access to tokens that come after the current token being predicted. The attention mask helps the model mask out the future tokens, ensuring that the predictions for a given token only depend on the tokens that precede it.\n",
        "\n",
        "By incorporating the attention mask, the model can attend to the relevant tokens and focus on the appropriate information while disregarding the padding tokens and future tokens. This enables more efficient and accurate modeling of the input sequence."
      ],
      "metadata": {
        "id": "Xffxc09Ab8XP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dq_WZ0MSmY9T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}